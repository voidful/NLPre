{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Feature \u00b6 handle over 100 dataset generate statistic report about processed dataset support many pre-processing ways Provide a panel for entering your parameters at runtime easy to adapt your own dataset and pre-processing utility Online Explorer \u00b6 https://voidful.github.io/NLPrep-Datasets/ Quick Start \u00b6 Installing via pip \u00b6 pip install nlprep get one of the dataset \u00b6 nlprep --dataset clas_udicstm --outdir sentiment --util You can also try nlprep in Google Colab: Overview \u00b6 $ nlprep arguments: --dataset which dataset to use --outdir processed result output directory optional arguments: -h, --help show this help message and exit --util data preprocessing utility, multiple utility are supported --cachedir dir for caching raw dataset --infile local dataset path --report generate a html statistics report Contributing \u00b6 Thanks for your interest.There are many ways to contribute to this project. Get started here . License \u00b6 License Icons reference \u00b6 Icons modify from Darius Dan from www.flaticon.com Icons modify from Freepik from www.flaticon.com","title":"Home"},{"location":"#feature","text":"handle over 100 dataset generate statistic report about processed dataset support many pre-processing ways Provide a panel for entering your parameters at runtime easy to adapt your own dataset and pre-processing utility","title":"Feature"},{"location":"#online-explorer","text":"https://voidful.github.io/NLPrep-Datasets/","title":"Online Explorer"},{"location":"#quick-start","text":"","title":"Quick Start"},{"location":"#installing-via-pip","text":"pip install nlprep","title":"Installing via pip"},{"location":"#get-one-of-the-dataset","text":"nlprep --dataset clas_udicstm --outdir sentiment --util You can also try nlprep in Google Colab:","title":"get one of the dataset"},{"location":"#overview","text":"$ nlprep arguments: --dataset which dataset to use --outdir processed result output directory optional arguments: -h, --help show this help message and exit --util data preprocessing utility, multiple utility are supported --cachedir dir for caching raw dataset --infile local dataset path --report generate a html statistics report","title":"Overview"},{"location":"#contributing","text":"Thanks for your interest.There are many ways to contribute to this project. Get started here .","title":"Contributing"},{"location":"#license","text":"License","title":"License"},{"location":"#icons-reference","text":"Icons modify from Darius Dan from www.flaticon.com Icons modify from Freepik from www.flaticon.com","title":"Icons reference"},{"location":"datasets/","text":"Browse All Available Dataset \u00b6 Online Explorer \u00b6 https://voidful.github.io/NLPrep-Datasets/ Add a new dataset \u00b6 follow template from template/dataset edit task_datasetname to your task. eg: /tag_clner edit dataset.py in template/dataset/task_datasetname Edit DATASETINFO DATASETINFO = { 'DATASET_FILE_MAP' : { \"dataset_name\" : \"dataset path\" # list for multiple detests in one tag }, 'TASK' : [ \"gen\" , \"tag\" , \"clas\" , \"qa\" ], 'FULLNAME' : \"Dataset Full Name\" , 'REF' : { \"Some dataset reference\" : \"useful link\" }, 'DESCRIPTION' : 'Dataset description' } Implement load for pre-loading 'DATASET_FILE_MAP' 's data def load ( data ): return data Implement toMiddleFormat for converting file to input and target def toMiddleFormat ( path ): dataset = MiddleFormat ( DATASETINFO ) # some file reading and processing dataset . add_data ( \"input\" , \"target\" ) return dataset move task_datasetname folder to nlprep/datasets","title":"Datasets"},{"location":"datasets/#browse-all-available-dataset","text":"","title":"Browse All Available Dataset"},{"location":"datasets/#online-explorer","text":"https://voidful.github.io/NLPrep-Datasets/","title":"Online Explorer"},{"location":"datasets/#add-a-new-dataset","text":"follow template from template/dataset edit task_datasetname to your task. eg: /tag_clner edit dataset.py in template/dataset/task_datasetname Edit DATASETINFO DATASETINFO = { 'DATASET_FILE_MAP' : { \"dataset_name\" : \"dataset path\" # list for multiple detests in one tag }, 'TASK' : [ \"gen\" , \"tag\" , \"clas\" , \"qa\" ], 'FULLNAME' : \"Dataset Full Name\" , 'REF' : { \"Some dataset reference\" : \"useful link\" }, 'DESCRIPTION' : 'Dataset description' } Implement load for pre-loading 'DATASET_FILE_MAP' 's data def load ( data ): return data Implement toMiddleFormat for converting file to input and target def toMiddleFormat ( path ): dataset = MiddleFormat ( DATASETINFO ) # some file reading and processing dataset . add_data ( \"input\" , \"target\" ) return dataset move task_datasetname folder to nlprep/datasets","title":"Add a new dataset"},{"location":"installation/","text":"Installation \u00b6 nlprep is tested on Python 3.6+, and PyTorch 1.1.0+. Installing via pip \u00b6 pip install nlprep Installing via source \u00b6 git clone https://github.com/voidful/nlprep.git python setup.py install Running nlprep \u00b6 Once you've installed nlprep, you can run with pip installed version: \u00b6 nlprep local version: \u00b6 python -m nlprep.main","title":"Installation"},{"location":"installation/#installation","text":"nlprep is tested on Python 3.6+, and PyTorch 1.1.0+.","title":"Installation"},{"location":"installation/#installing-via-pip","text":"pip install nlprep","title":"Installing via pip"},{"location":"installation/#installing-via-source","text":"git clone https://github.com/voidful/nlprep.git python setup.py install","title":"Installing via source"},{"location":"installation/#running-nlprep","text":"Once you've installed nlprep, you can run with","title":"Running nlprep"},{"location":"installation/#pip-installed-version","text":"nlprep","title":"pip installed version:"},{"location":"installation/#local-version","text":"python -m nlprep.main","title":"local version:"},{"location":"usage/","text":"Usage \u00b6 Overview \u00b6 $ nlprep arguments: --dataset which dataset to use --outdir processed result output directory optional arguments: -h, --help show this help message and exit --util data preprocessing utility, multiple utility are supported --cachedir dir for caching raw dataset --infile local dataset path --report generate a html statistics report Python \u00b6 import os import nlprep datasets = nlprep . list_all_datasets () ds = nlprep . load_dataset ( datasets [ 0 ]) ds_info = ds . DATASETINFO for ds_name , mf in nlprep . convert_middleformat ( ds ) . items (): print ( ds_name , ds_info , mf . dump_list ()[: 3 ]) profile = mf . get_report ( ds_name ) profile . to_file ( os . path . join ( './' , ds_name + \"_report.html\" )) Example \u00b6 Download udicstm dataset that nlprep --dataset clas_udicstm --outdir sentiment --util splitData --report Show result file !head -10 ./sentiment/udicstm_valid.csv \u6703\u751f\u5b69\u5b50\u4e0d\u7b49\u65bc\u6703\u7576\u7236\u6bcd\uff0c\u9019\u53ef\u80fd\u8b93\u8a31\u591a\u4eba\u7121\u6cd5\u63a5\u53d7\uff0c\u4e0d\u5c11\u7236\u6bcd\u6253\u7740\u201c\u611b\u5b69\u5b50\u201d\u7684\u65d7\u865f\u505a\u4e86\u8a31\u591a\u963b\u7919\u5b69\u5b50\u5fc3\u667a\u767c\u5c55\u7684\u4e8b\uff0c\u751a\u81f3\u50b7\u5bb3\u4e86\u5b69\u5b50\u537b\u9084\u4e0d\u77e5\u9053\uff0c\u53cd\u800c\u602a\u5b69\u5b50\u3002\u770b\u4e86\u9019\u672c\u66f8\u6211\u6df1\u53d7\u6559\u80b2\uff0c\u6211\u6176\u5e78\u5728\u5bf6\u5bf6\u624d\u4e03\u500b\u6708\u5c31\u770b\u5230\u4e86\u9019\u672c\u66f8\uff0c\u800c\u4e0d\u662f\u4e03\u6b72\u6216\u8005\u5341\u4e03\u6b72\uff0c\u53ef\u80fd\u6703\u8b93\u6211\u5728\u6559\u80b2\u5b69\u5b50\u65b9\u9762\u5c11\u8d70\u8a31\u591a\u5f4e\u8def\u3002\u975e\u5e38\u611f\u8b1d\u5c39\u5efa\u8389\u8001\u5e2b\uff0c\u5e0c\u671b\u5979\u518d\u5beb\u51fa\u66f4\u597d\u7684\u66f8\u3002\u4e5f\u5e0c\u671b\u8846\u591a\u7684\u5e74\u8f15\u7236\u6bcd\u597d\u597d\u770b\u770b\u9019\u672c\u66f8\u3002\u6211\u5df2\u5411\u8a31\u591a\u670b\u53cb\u63a8\u85a6\u6b64\u66f8\u3002,positive \u7b2c\u4e00\uff0c\u4e00\u63d2\u5165\u7121\u7dda\u4e0a\u7db2\u5361\uff08usb\u63a5\u53e3\uff09\u5c31\u81ea\u52d5\u95dc\u6a5f\uff1b\u7b2c\u4e8c\uff0c\u5f85\u6a5f\u6642\u9593\u6c92\u6709\u5ba3\u7a31\u7684\u90a3\u9ebc\u9577\u4e45\uff1b\u7b2c\u4e09\uff0c\u6bd4\u8f03\u5bb9\u6613\u6cbe\u624b\u5370\u3002,negative \"\u5c0f\u5de7\u5be6\u7528,\u5916\u89c0\u597d\u770b;\u800c\u4e14\u7cfb\u7d71\u76e4\u6240\u5728\u7684\u5340\u548c\u5176\u5b83\u5340\u5df2\u7d93\u5206\u958b,\u5118\u7ba1\u53ea\u6709\u5169\u500b\u5340,\u4e0d\u904e\u5df2\u7d93\u8db3\u5920\u4e86\",positive \u7279\u50f9\u623f\u975e\u5e38\u5c0f \u56db\u6b65\u8d70\u5230\u623f\u9593\u7246\u89d2 \u57fa\u672c\u662f\u7528\u4e0d\u9694\u97f3\u7684\u677f\u6750\u9694\u51fa\u4f86\u7684 \u9694\u58c1\u7684\u96fb\u8996\u8072\u97f3 \u9084\u6709\u81e8\u8fd1\u623f\u9593\u591c\u665a\u7537\u5973\u505a\u4e8b\u7684\u547b\u541f\u548c\u540c\u6d74\u7684\u8072\u97f3\u90fd\u80fd\u5f88\u6e05\u695a\u7684\u807d\u898b \u7c21\u76f4\u5c31\u662f\u7db2\u53cb\u898b\u9762\u7684\u70ae\u623f \u623f\u9593\u88cf\u7a7a\u6c23\u8cea\u91cf\u5f88\u5dee \u4e14\u7121\u6cd5\u901a\u904e\u63db\u6c23\u6392\u51fa \u651c\u7a0b\u50f9\u683c\u8207\u9580\u5e02\u50f9\u76f8\u540c \u4e3b\u8981\u8003\u616e\u8fa6\u4e8b\u5730\u9ede\u5728\u9644\u8fd1 \u7e94\u53bb\u4f4f\u7684,negative \u5728\u540c\u7b49\u50f9\u4f4d\u4e0a\u4f86\u8b1b\u914d\u7f6e\u4e0d\u932f\uff0c\u54c1\u724c\u77e5\u540d\u5ea6\u9ad8\uff0c\u54c1\u8cea\u4e5f\u6709\u4fdd\u8b49\u3002\u5546\u52d9\u6a5f\u578b\uff0c\u5916\u89c0\u4e00\u822c\uff0c\u6309\u9375\u624b\u611f\u5f88\u597d\uff0c\u6234\u723e\u7684\u96fb\u6e90\u9069\u914d\u5668\u9020\u578b\u5f88\u597d\uff0c\u4e5f\u6bd4\u8f03\u8f15\u5de7\u3002,positive \u4e00\u822c\u7684\u66f8\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002,negative \"\u6709\u9ede\u91cd\uff0c\u662f\u500b\u907a\u61be\u3002\u80fd\u8cb7\u9019\u9ebc\u5c0f\u7684\u7b46\u8a18\u672c\uff0c\u5c31\u662f\u5e0c\u671b\u53ef\u4ee5\u65b9\u4fbf\u651c\u5e36\u3002\u5c3a\u5bf8\u662fOK\u4e86\uff0c\u8981\u662f\u518d\u8f15\u8584\u4e9b\u5c31\u66f4\u5b8c\u7f8e\u4e86\u3002\u6c92\u6709\u5149\u9a45\u7684\u8aaa\uff0c\u6240\u4ee5\u83ef\u78a9\u6709\u5f85\u6539\u5584\u3002\u7136\u5f8c\u5c31\u662f\u5916\u6bbc\u96d6\u7136\u662f\u70e4\u6f06\u7684\uff0c\u5f88\u6f02\u4eae\uff08\u8acb\u52ff\u89f8\u6478\uff09,\u56e0\u7232\u4e00\u89f8\u6478\u5c31\u6703\u7559\u4e0b\u6307\u7d0b\",negative \u81ea\u5e36\u4e86\u4e00\u500b\u767d\u8272\u7684\u5305\u5305\uff0c\u4e0d\u7528\u984d\u5916\u8cb7\u4e86,positive \"\u525b\u6536\u5230,\u767c\u73fe\u9375\u76e4\u6709\u4e9b\u9b06,\u89f8\u6478\u5c4f\u592a\u96e3\u6309\u4e86,\u6700\u4e3b\u8981\u7684\u662f\u958b\u6a5f\u7684\u6642\u5019\u6253\u958b\u548c\u95dc\u4e0a\u5149\u9a45\u5c0e\u81f4\u7cfb\u7d71\u85cd\u5c4f,\u4e0d\u77e5\u9053\u662f\u4e0d\u662f\u9019\u500b\u539f\u56e0 , \u5176\u4ed6\u7684\u5230\u76ee\u524d\u7232\u6b62\u6b63\u5e38.\",negative \"\u9152\u5e97\u5730\u7406\u4f4d\u7f6e\u4e0d\u932f,\u9580\u53e3\u6642\u9ad8\u901f\u548c\u8f15\u8ecc.\",negative Report will be at sentiment/udicstm_valid_report.html","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#overview","text":"$ nlprep arguments: --dataset which dataset to use --outdir processed result output directory optional arguments: -h, --help show this help message and exit --util data preprocessing utility, multiple utility are supported --cachedir dir for caching raw dataset --infile local dataset path --report generate a html statistics report","title":"Overview"},{"location":"usage/#python","text":"import os import nlprep datasets = nlprep . list_all_datasets () ds = nlprep . load_dataset ( datasets [ 0 ]) ds_info = ds . DATASETINFO for ds_name , mf in nlprep . convert_middleformat ( ds ) . items (): print ( ds_name , ds_info , mf . dump_list ()[: 3 ]) profile = mf . get_report ( ds_name ) profile . to_file ( os . path . join ( './' , ds_name + \"_report.html\" ))","title":"Python"},{"location":"usage/#example","text":"Download udicstm dataset that nlprep --dataset clas_udicstm --outdir sentiment --util splitData --report Show result file !head -10 ./sentiment/udicstm_valid.csv \u6703\u751f\u5b69\u5b50\u4e0d\u7b49\u65bc\u6703\u7576\u7236\u6bcd\uff0c\u9019\u53ef\u80fd\u8b93\u8a31\u591a\u4eba\u7121\u6cd5\u63a5\u53d7\uff0c\u4e0d\u5c11\u7236\u6bcd\u6253\u7740\u201c\u611b\u5b69\u5b50\u201d\u7684\u65d7\u865f\u505a\u4e86\u8a31\u591a\u963b\u7919\u5b69\u5b50\u5fc3\u667a\u767c\u5c55\u7684\u4e8b\uff0c\u751a\u81f3\u50b7\u5bb3\u4e86\u5b69\u5b50\u537b\u9084\u4e0d\u77e5\u9053\uff0c\u53cd\u800c\u602a\u5b69\u5b50\u3002\u770b\u4e86\u9019\u672c\u66f8\u6211\u6df1\u53d7\u6559\u80b2\uff0c\u6211\u6176\u5e78\u5728\u5bf6\u5bf6\u624d\u4e03\u500b\u6708\u5c31\u770b\u5230\u4e86\u9019\u672c\u66f8\uff0c\u800c\u4e0d\u662f\u4e03\u6b72\u6216\u8005\u5341\u4e03\u6b72\uff0c\u53ef\u80fd\u6703\u8b93\u6211\u5728\u6559\u80b2\u5b69\u5b50\u65b9\u9762\u5c11\u8d70\u8a31\u591a\u5f4e\u8def\u3002\u975e\u5e38\u611f\u8b1d\u5c39\u5efa\u8389\u8001\u5e2b\uff0c\u5e0c\u671b\u5979\u518d\u5beb\u51fa\u66f4\u597d\u7684\u66f8\u3002\u4e5f\u5e0c\u671b\u8846\u591a\u7684\u5e74\u8f15\u7236\u6bcd\u597d\u597d\u770b\u770b\u9019\u672c\u66f8\u3002\u6211\u5df2\u5411\u8a31\u591a\u670b\u53cb\u63a8\u85a6\u6b64\u66f8\u3002,positive \u7b2c\u4e00\uff0c\u4e00\u63d2\u5165\u7121\u7dda\u4e0a\u7db2\u5361\uff08usb\u63a5\u53e3\uff09\u5c31\u81ea\u52d5\u95dc\u6a5f\uff1b\u7b2c\u4e8c\uff0c\u5f85\u6a5f\u6642\u9593\u6c92\u6709\u5ba3\u7a31\u7684\u90a3\u9ebc\u9577\u4e45\uff1b\u7b2c\u4e09\uff0c\u6bd4\u8f03\u5bb9\u6613\u6cbe\u624b\u5370\u3002,negative \"\u5c0f\u5de7\u5be6\u7528,\u5916\u89c0\u597d\u770b;\u800c\u4e14\u7cfb\u7d71\u76e4\u6240\u5728\u7684\u5340\u548c\u5176\u5b83\u5340\u5df2\u7d93\u5206\u958b,\u5118\u7ba1\u53ea\u6709\u5169\u500b\u5340,\u4e0d\u904e\u5df2\u7d93\u8db3\u5920\u4e86\",positive \u7279\u50f9\u623f\u975e\u5e38\u5c0f \u56db\u6b65\u8d70\u5230\u623f\u9593\u7246\u89d2 \u57fa\u672c\u662f\u7528\u4e0d\u9694\u97f3\u7684\u677f\u6750\u9694\u51fa\u4f86\u7684 \u9694\u58c1\u7684\u96fb\u8996\u8072\u97f3 \u9084\u6709\u81e8\u8fd1\u623f\u9593\u591c\u665a\u7537\u5973\u505a\u4e8b\u7684\u547b\u541f\u548c\u540c\u6d74\u7684\u8072\u97f3\u90fd\u80fd\u5f88\u6e05\u695a\u7684\u807d\u898b \u7c21\u76f4\u5c31\u662f\u7db2\u53cb\u898b\u9762\u7684\u70ae\u623f \u623f\u9593\u88cf\u7a7a\u6c23\u8cea\u91cf\u5f88\u5dee \u4e14\u7121\u6cd5\u901a\u904e\u63db\u6c23\u6392\u51fa \u651c\u7a0b\u50f9\u683c\u8207\u9580\u5e02\u50f9\u76f8\u540c \u4e3b\u8981\u8003\u616e\u8fa6\u4e8b\u5730\u9ede\u5728\u9644\u8fd1 \u7e94\u53bb\u4f4f\u7684,negative \u5728\u540c\u7b49\u50f9\u4f4d\u4e0a\u4f86\u8b1b\u914d\u7f6e\u4e0d\u932f\uff0c\u54c1\u724c\u77e5\u540d\u5ea6\u9ad8\uff0c\u54c1\u8cea\u4e5f\u6709\u4fdd\u8b49\u3002\u5546\u52d9\u6a5f\u578b\uff0c\u5916\u89c0\u4e00\u822c\uff0c\u6309\u9375\u624b\u611f\u5f88\u597d\uff0c\u6234\u723e\u7684\u96fb\u6e90\u9069\u914d\u5668\u9020\u578b\u5f88\u597d\uff0c\u4e5f\u6bd4\u8f03\u8f15\u5de7\u3002,positive \u4e00\u822c\u7684\u66f8\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002,negative \"\u6709\u9ede\u91cd\uff0c\u662f\u500b\u907a\u61be\u3002\u80fd\u8cb7\u9019\u9ebc\u5c0f\u7684\u7b46\u8a18\u672c\uff0c\u5c31\u662f\u5e0c\u671b\u53ef\u4ee5\u65b9\u4fbf\u651c\u5e36\u3002\u5c3a\u5bf8\u662fOK\u4e86\uff0c\u8981\u662f\u518d\u8f15\u8584\u4e9b\u5c31\u66f4\u5b8c\u7f8e\u4e86\u3002\u6c92\u6709\u5149\u9a45\u7684\u8aaa\uff0c\u6240\u4ee5\u83ef\u78a9\u6709\u5f85\u6539\u5584\u3002\u7136\u5f8c\u5c31\u662f\u5916\u6bbc\u96d6\u7136\u662f\u70e4\u6f06\u7684\uff0c\u5f88\u6f02\u4eae\uff08\u8acb\u52ff\u89f8\u6478\uff09,\u56e0\u7232\u4e00\u89f8\u6478\u5c31\u6703\u7559\u4e0b\u6307\u7d0b\",negative \u81ea\u5e36\u4e86\u4e00\u500b\u767d\u8272\u7684\u5305\u5305\uff0c\u4e0d\u7528\u984d\u5916\u8cb7\u4e86,positive \"\u525b\u6536\u5230,\u767c\u73fe\u9375\u76e4\u6709\u4e9b\u9b06,\u89f8\u6478\u5c4f\u592a\u96e3\u6309\u4e86,\u6700\u4e3b\u8981\u7684\u662f\u958b\u6a5f\u7684\u6642\u5019\u6253\u958b\u548c\u95dc\u4e0a\u5149\u9a45\u5c0e\u81f4\u7cfb\u7d71\u85cd\u5c4f,\u4e0d\u77e5\u9053\u662f\u4e0d\u662f\u9019\u500b\u539f\u56e0 , \u5176\u4ed6\u7684\u5230\u76ee\u524d\u7232\u6b62\u6b63\u5e38.\",negative \"\u9152\u5e97\u5730\u7406\u4f4d\u7f6e\u4e0d\u932f,\u9580\u53e3\u6642\u9ad8\u901f\u548c\u8f15\u8ecc.\",negative Report will be at sentiment/udicstm_valid_report.html","title":"Example"},{"location":"utility/","text":"\u00b6 pairslevel \u00b6 reverse ( path , pair ) \u00b6 swap input and target data Source code in nlprep/utils/pairslevel.py 107 108 109 110 111 def reverse ( path , pair ): \"\"\"swap input and target data\"\"\" for p in pair : p . reverse () return [[ path , pair ]] rmAllSameTag ( path , pair ) \u00b6 remove all same tag in tagging dataset Source code in nlprep/utils/pairslevel.py 98 99 100 101 102 103 104 def rmAllSameTag ( path , pair ): \"\"\"remove all same tag in tagging dataset\"\"\" result_pair = [] for p in pair : if len ( set ( p [ 1 ])) > 1 : result_pair . append ( p ) return [[ path , result_pair ]] setAllSameTagRate ( path , pair , seed = 612 , rate = 0.27 ) \u00b6 set all same tag data ratio in tagging dataset Source code in nlprep/utils/pairslevel.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def setAllSameTagRate ( path , pair , seed = 612 , rate = 0.27 ): \"\"\"set all same tag data ratio in tagging dataset\"\"\" random . seed ( seed ) allsame_pair = [] notsame_pair = [] for p in pair : if len ( set ( p [ 1 ])) < 2 : allsame_pair . append ( p ) else : notsame_pair . append ( p ) asnum = min ( int ( len ( notsame_pair ) * rate ), len ( allsame_pair )) print ( \"all same pair:\" , len ( allsame_pair ), \"have diff pair:\" , len ( notsame_pair ), \"ratio:\" , rate , \"take:\" , asnum ) random . shuffle ( allsame_pair ) result = allsame_pair [: asnum ] + notsame_pair random . shuffle ( result ) return [[ path , result ]] setMaxLen ( path , pair , maxlen = 512 , tokenizer = 'word' , with_target = [ True , False ], handle_over = [ 'remove' , 'slice' ]) \u00b6 set model maximum length Source code in nlprep/utils/pairslevel.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def setMaxLen ( path , pair , maxlen = 512 , tokenizer = \"word\" , with_target = [ True , False ], handle_over = [ 'remove' , 'slice' ]): \"\"\"set model maximum length\"\"\" global separate_token maxlen = int ( maxlen ) if tokenizer == 'word' : sep_func = nlp2 . split_sentence_to_array elif tokenizer == 'char' : sep_func = list else : if 'voidful/albert' in tokenizer : tok = BertTokenizer . from_pretrained ( tokenizer ) else : tok = AutoTokenizer . from_pretrained ( tokenizer ) sep_func = tok . tokenize new_sep_token = \" \" . join ( sep_func ( separate_token )) . strip () small_than_max_pairs = [] for ind , p in enumerate ( pair ): tok_input = sep_func ( p [ 0 ] + \" \" + p [ 1 ]) if with_target else sep_func ( p [ 0 ]) if len ( tok_input ) < maxlen : small_than_max_pairs . append ( p ) elif handle_over == 'slice' : exceed = len ( tok_input ) - maxlen + 3 # +3 for more flexible space to further avoid exceed first_sep_index = tok_input . index ( new_sep_token ) if new_sep_token in tok_input else len ( tok_input ) limit_len = first_sep_index - exceed if limit_len > 0 : tok_input = tok_input [: limit_len ] + tok_input [ first_sep_index :] if tokenizer == 'char' : small_than_max_pairs . append ([( \"\" . join ( tok_input )) . replace ( new_sep_token , separate_token ), p [ 1 ]]) else : small_than_max_pairs . append ([( \" \" . join ( tok_input )) . replace ( new_sep_token , separate_token ), p [ 1 ]]) print ( \"Num of data before handle max len :\" , len ( pair )) print ( \"Num of data after handle max len :\" , len ( small_than_max_pairs )) return [[ path , small_than_max_pairs ]] setSepToken ( path , pair , sep_token = '[SEP]' ) \u00b6 set SEP token for different pre-trained model Source code in nlprep/utils/pairslevel.py 34 35 36 37 38 39 40 41 def setSepToken ( path , pair , sep_token = \"[SEP]\" ): \"\"\"set SEP token for different pre-trained model\"\"\" global separate_token separate_token = sep_token for ind , p in enumerate ( pair ): input_sent = p [ 0 ] pair [ ind ][ 0 ] = input_sent . replace ( \"[SEP]\" , sep_token ) return [[ path , pair ]] splitData ( path , pair , seed = 612 , train_ratio = 0.7 , test_ratio = 0.2 , valid_ratio = 0.1 ) \u00b6 split data into training testing and validation Source code in nlprep/utils/pairslevel.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def splitData ( path , pair , seed = 612 , train_ratio = 0.7 , test_ratio = 0.2 , valid_ratio = 0.1 ): \"\"\"split data into training testing and validation\"\"\" random . seed ( seed ) random . shuffle ( pair ) train_ratio = float ( train_ratio ) test_ratio = float ( test_ratio ) valid_ratio = float ( valid_ratio ) assert train_ratio > test_ratio >= valid_ratio and round ( train_ratio + test_ratio + valid_ratio ) == 1.0 train_num = int ( len ( pair ) * train_ratio ) test_num = train_num + int ( len ( pair ) * test_ratio ) valid_num = test_num + int ( len ( pair ) * valid_ratio ) return [[ path + \"_train\" , pair [: train_num ]], [ path + \"_test\" , pair [ train_num : test_num ]], [ path + \"_valid\" , pair [ test_num : valid_num ]]] splitDataIntoPart ( path , pair , seed = 712 , part = 4 ) \u00b6 split data into part because of not enough memory Source code in nlprep/utils/pairslevel.py 26 27 28 29 30 31 def splitDataIntoPart ( path , pair , seed = 712 , part = 4 ): \"\"\"split data into part because of not enough memory\"\"\" random . seed ( seed ) random . shuffle ( pair ) part = int ( len ( pair ) / part ) + 1 return [[ path + \"_\" + str ( int ( i / part )), pair [ i : i + part ]] for i in range ( 0 , len ( pair ), part )] sentlevel \u00b6 s2t ( convt ) \u00b6 simplify chines to traditional chines Source code in nlprep/utils/sentlevel.py 10 11 12 def s2t ( convt ): \"\"\"simplify chines to traditional chines\"\"\" return cc_s2t . convert ( convt ) t2s ( convt ) \u00b6 traditional chines to simplify chines Source code in nlprep/utils/sentlevel.py 15 16 17 def t2s ( convt ): \"\"\"traditional chines to simplify chines\"\"\" return cc_t2s . convert ( convt ) Add a new utility \u00b6 sentence level: add function into utils/sentlevel.py, function name will be --util parameter paris level - add function into utils/parislevel.py, function name will be --util parameter","title":"Utilities"},{"location":"utility/#nlprep.utils","text":"","title":"nlprep.utils"},{"location":"utility/#nlprep.utils.pairslevel","text":"","title":"pairslevel"},{"location":"utility/#nlprep.utils.pairslevel.reverse","text":"swap input and target data Source code in nlprep/utils/pairslevel.py 107 108 109 110 111 def reverse ( path , pair ): \"\"\"swap input and target data\"\"\" for p in pair : p . reverse () return [[ path , pair ]]","title":"reverse()"},{"location":"utility/#nlprep.utils.pairslevel.rmAllSameTag","text":"remove all same tag in tagging dataset Source code in nlprep/utils/pairslevel.py 98 99 100 101 102 103 104 def rmAllSameTag ( path , pair ): \"\"\"remove all same tag in tagging dataset\"\"\" result_pair = [] for p in pair : if len ( set ( p [ 1 ])) > 1 : result_pair . append ( p ) return [[ path , result_pair ]]","title":"rmAllSameTag()"},{"location":"utility/#nlprep.utils.pairslevel.setAllSameTagRate","text":"set all same tag data ratio in tagging dataset Source code in nlprep/utils/pairslevel.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def setAllSameTagRate ( path , pair , seed = 612 , rate = 0.27 ): \"\"\"set all same tag data ratio in tagging dataset\"\"\" random . seed ( seed ) allsame_pair = [] notsame_pair = [] for p in pair : if len ( set ( p [ 1 ])) < 2 : allsame_pair . append ( p ) else : notsame_pair . append ( p ) asnum = min ( int ( len ( notsame_pair ) * rate ), len ( allsame_pair )) print ( \"all same pair:\" , len ( allsame_pair ), \"have diff pair:\" , len ( notsame_pair ), \"ratio:\" , rate , \"take:\" , asnum ) random . shuffle ( allsame_pair ) result = allsame_pair [: asnum ] + notsame_pair random . shuffle ( result ) return [[ path , result ]]","title":"setAllSameTagRate()"},{"location":"utility/#nlprep.utils.pairslevel.setMaxLen","text":"set model maximum length Source code in nlprep/utils/pairslevel.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def setMaxLen ( path , pair , maxlen = 512 , tokenizer = \"word\" , with_target = [ True , False ], handle_over = [ 'remove' , 'slice' ]): \"\"\"set model maximum length\"\"\" global separate_token maxlen = int ( maxlen ) if tokenizer == 'word' : sep_func = nlp2 . split_sentence_to_array elif tokenizer == 'char' : sep_func = list else : if 'voidful/albert' in tokenizer : tok = BertTokenizer . from_pretrained ( tokenizer ) else : tok = AutoTokenizer . from_pretrained ( tokenizer ) sep_func = tok . tokenize new_sep_token = \" \" . join ( sep_func ( separate_token )) . strip () small_than_max_pairs = [] for ind , p in enumerate ( pair ): tok_input = sep_func ( p [ 0 ] + \" \" + p [ 1 ]) if with_target else sep_func ( p [ 0 ]) if len ( tok_input ) < maxlen : small_than_max_pairs . append ( p ) elif handle_over == 'slice' : exceed = len ( tok_input ) - maxlen + 3 # +3 for more flexible space to further avoid exceed first_sep_index = tok_input . index ( new_sep_token ) if new_sep_token in tok_input else len ( tok_input ) limit_len = first_sep_index - exceed if limit_len > 0 : tok_input = tok_input [: limit_len ] + tok_input [ first_sep_index :] if tokenizer == 'char' : small_than_max_pairs . append ([( \"\" . join ( tok_input )) . replace ( new_sep_token , separate_token ), p [ 1 ]]) else : small_than_max_pairs . append ([( \" \" . join ( tok_input )) . replace ( new_sep_token , separate_token ), p [ 1 ]]) print ( \"Num of data before handle max len :\" , len ( pair )) print ( \"Num of data after handle max len :\" , len ( small_than_max_pairs )) return [[ path , small_than_max_pairs ]]","title":"setMaxLen()"},{"location":"utility/#nlprep.utils.pairslevel.setSepToken","text":"set SEP token for different pre-trained model Source code in nlprep/utils/pairslevel.py 34 35 36 37 38 39 40 41 def setSepToken ( path , pair , sep_token = \"[SEP]\" ): \"\"\"set SEP token for different pre-trained model\"\"\" global separate_token separate_token = sep_token for ind , p in enumerate ( pair ): input_sent = p [ 0 ] pair [ ind ][ 0 ] = input_sent . replace ( \"[SEP]\" , sep_token ) return [[ path , pair ]]","title":"setSepToken()"},{"location":"utility/#nlprep.utils.pairslevel.splitData","text":"split data into training testing and validation Source code in nlprep/utils/pairslevel.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def splitData ( path , pair , seed = 612 , train_ratio = 0.7 , test_ratio = 0.2 , valid_ratio = 0.1 ): \"\"\"split data into training testing and validation\"\"\" random . seed ( seed ) random . shuffle ( pair ) train_ratio = float ( train_ratio ) test_ratio = float ( test_ratio ) valid_ratio = float ( valid_ratio ) assert train_ratio > test_ratio >= valid_ratio and round ( train_ratio + test_ratio + valid_ratio ) == 1.0 train_num = int ( len ( pair ) * train_ratio ) test_num = train_num + int ( len ( pair ) * test_ratio ) valid_num = test_num + int ( len ( pair ) * valid_ratio ) return [[ path + \"_train\" , pair [: train_num ]], [ path + \"_test\" , pair [ train_num : test_num ]], [ path + \"_valid\" , pair [ test_num : valid_num ]]]","title":"splitData()"},{"location":"utility/#nlprep.utils.pairslevel.splitDataIntoPart","text":"split data into part because of not enough memory Source code in nlprep/utils/pairslevel.py 26 27 28 29 30 31 def splitDataIntoPart ( path , pair , seed = 712 , part = 4 ): \"\"\"split data into part because of not enough memory\"\"\" random . seed ( seed ) random . shuffle ( pair ) part = int ( len ( pair ) / part ) + 1 return [[ path + \"_\" + str ( int ( i / part )), pair [ i : i + part ]] for i in range ( 0 , len ( pair ), part )]","title":"splitDataIntoPart()"},{"location":"utility/#nlprep.utils.sentlevel","text":"","title":"sentlevel"},{"location":"utility/#nlprep.utils.sentlevel.s2t","text":"simplify chines to traditional chines Source code in nlprep/utils/sentlevel.py 10 11 12 def s2t ( convt ): \"\"\"simplify chines to traditional chines\"\"\" return cc_s2t . convert ( convt )","title":"s2t()"},{"location":"utility/#nlprep.utils.sentlevel.t2s","text":"traditional chines to simplify chines Source code in nlprep/utils/sentlevel.py 15 16 17 def t2s ( convt ): \"\"\"traditional chines to simplify chines\"\"\" return cc_t2s . convert ( convt )","title":"t2s()"},{"location":"utility/#add-a-new-utility","text":"sentence level: add function into utils/sentlevel.py, function name will be --util parameter paris level - add function into utils/parislevel.py, function name will be --util parameter","title":"Add a new utility"}]}